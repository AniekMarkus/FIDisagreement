{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules / get functions in other Python scripts\n",
    "import app.results_explorer_utils as drc\n",
    "import app.results_explorer_figures as figs\n",
    "from app.results_explorer_helpers import *\n",
    "\n",
    "from app.results_explorer_input import output_folder, color_dict\n",
    "# TODO: set output_path in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Figures in notebook\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Experiments 1:\n",
    "fimethod=['permutation_auc', 'permutation_mse', 'loco_auc', 'loco_mse', 'kernelshap', 'sage_marginal', 'sage_conditional']\n",
    "eval_metrics = ['overlap', 'sign', 'ranksign', 'kendalltau', 'mae', 'rmse']\n",
    "top_metrics = ['overlap', 'sign', 'ranksign']  # can be compared between data and models, considers top variables\n",
    "# order_metrics = ['kendalltau'] # can be compared between data and models, considers ordering variables\n",
    "value_metrics = ['mae', 'rmse'] # are normalised, can only be compared within each plot (model + data combi)\n",
    "\n",
    "for d in [\"compas\", \"german\", \"heartfailurestroke\", \"copdmortality\"]:\n",
    "    for m in [\"logistic\" ,\"nn\"]:\n",
    "        for v in [\"v0\"]:\n",
    "            # figs.fi_values(output_folder, color_dict, d, v, m, fimethod)\n",
    "            # figs.fi_ranking(output_folder, color_dict, d, v, m, fimethod)\n",
    "            # figs.fi_topfeatures(output_folder, color_dict, d, v, m, fimethod, k=5)\n",
    "\n",
    "            # figs.fi_metrics(output_folder, \"single\", color_dict, d, v, m, fimethod, eval_metrics, False) # single metrics\n",
    "            # figs.fi_metrics(output_folder, \"all\", color_dict, d, v, m, fimethod, eval_metrics, True) # group all metrics\n",
    "\n",
    "            figs.heatmap_disagreement(output_folder, \"top\", d, v, m, fimethod, top_metrics) # heatmap top metrics\n",
    "            figs.heatmap_disagreement(output_folder, \"value\", d, v, m, fimethod, value_metrics) # heatmap value metrics\n",
    "\n",
    "            for metric in eval_metrics:\n",
    "                figs.heatmap_disagreement(output_folder, str(metric), d, v, m, fimethod, metric) # heatmap single metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Manual run of figures\n",
    "dataset=\"compas\"\n",
    "version=\"v0\"\n",
    "model=\"logistic\"\n",
    "eval_metrics = ['overlap', 'sign', 'ranksign', 'kendalltau', 'mae', 'rmse']\n",
    "fimethod=['permutation_auc', 'permutation_mse', 'loco_auc', 'loco_mse', 'kernelshap', 'sage_marginal', 'sage_conditional']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get feature importance values and ranking\n",
    "fi_rank, fi_values, settings_data = get_rank(output_folder, dataset, version, model, fimethod, scale=True)\n",
    "\n",
    "# Combine data\n",
    "# fi_rank = combine_fi(fi_rank, settings_data)\n",
    "# fi_values = combine_fi(fi_values, settings_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(fi_rank)\n",
    "print(fi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_plot = figs.fi_values(output_folder, color_dict, dataset, version, model, fimethod)\n",
    "fi_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_rank = figs.fi_ranking(output_folder, color_dict, dataset, version, model, fimethod)\n",
    "fi_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_top = figs.fi_topfeatures(output_folder, color_dict, dataset, version, model, fimethod, k=5)\n",
    "fi_top.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_metrics = figs.fi_metrics(output_folder, \"single\", color_dict, dataset, version, model, fimethod, eval_metrics, summarize=False)\n",
    "fi_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_metrics = figs.fi_metrics(output_folder, \"group\", color_dict, dataset, version, model, fimethod, eval_metrics, summarize=True)\n",
    "fi_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fi_heatmap = figs.heatmap_disagreement(output_folder, \"top\", dataset, version, model, fimethod, eval_metrics)\n",
    "fi_heatmap.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}